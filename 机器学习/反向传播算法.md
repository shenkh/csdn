# **1. 变量定义**

![img](https://img-blog.csdn.net/20160401202509000)
上图是一个三层人工神经网络，layer1至layer3分别是输入层、隐藏层和输出层。如图，先定义一些变量：

$w_{j k}^{l}$表示第$(l-1)$层的第$(k)$个神经元连接到第$l$层的第$j$个神经元的权重；

$b_{j}^{l}$表示第$l$层的第$j$个神经元的偏置；

$z_{j}^{l}$表示第$l$层的第$j$个神经元的输入，即：
$$
z_{j}^{l}=\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}
$$
$a_{j}^{l}$表示第$l$层的第$j$个神经元的输出，即：
$$
a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)
$$
其中$\sigma$表示激活函数。

# **2. 代价函数**

代价函数被用来计算ANN输出值与实际值之间的误差。常用的代价函数是二次代价函数（Quadratic cost function）：

$$
C=\frac{1}{2 n} \sum_{x}\left\|y(x)-a^{L}(x)\right\|^{2}
$$

其中，$x$表示输入的样本，$y$表示实际的分类，$a^{L}$表示预测的输出，$L$表示神经网络的最大层数。

# **3. 公式及其推导**

首先，将第$l$层第$j$个神经元中产生的错误（即实际值与预测值之间的误差）定义为：

$$
\delta_{j}^{l} \equiv \frac{\partial C}{\partial z_{j}^{l}}
$$

本文将以一个输入样本为例进行说明，此时代价函数表示为：

$$
C=\frac{1}{2}\left\|y-a^{L}\right\|^{2}=\frac{1}{2} \sum_{j}\left(y_{j}-a_{j}^{L}\right)^{2}
$$

**公式1（计算最后一层神经网络产生的错误）：**

$$
\delta^{L}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right)
$$

 其中，$\odot$表示Hadamard乘积，用于矩阵或向量之间点对点的乘法运算。公式1的推导过程如下：

$$
\because \delta_{j}^{L}=\frac{\partial C}{\partial z_{j}^{L}}=\frac{\partial C}{\partial a_{j}^{L}} \cdot \frac{\partial a_{j}^{L}}{\partial z_{j}^{L}}
$$

$$
\therefore \delta^{L}=\frac{\partial C}{\partial a^{L}} \odot \frac{\partial a^{L}}{\partial z^{L}}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right)
$$

**公式2（由后往前，计算每一层神经网络产生的错误）：**

$$
\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)
$$

推导过程：

$$
\begin{aligned} \because \delta_{j}^{l}=\frac{\partial C}{\partial z_{j}^{l}} &=\sum_{k} \frac{\partial C}{\partial z_{k}^{l+1}} \cdot \frac{\partial z_{k}^{l+1}}{\partial a_{j}^{l}} \cdot \frac{\partial a_{j}^{l}}{\partial z_{j}^{l}} \\ &=\sum_{k} \delta_{k}^{l+1} \cdot \frac{\partial\left(w_{k j}^{l+1} a_{j}^{l}+b_{k}^{l+1}\right)}{\partial a_{j}^{l}} \cdot \sigma^{\prime}\left(z_{j}^{l}\right) \\ &=\sum_{k} \delta_{k}^{l+1} \cdot w_{k j}^{l+1} \cdot \sigma^{\prime}\left(z_{j}^{l}\right) \end{aligned}
$$

$$
\therefore \delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)
$$

**公式3（计算权重的梯度）：**

$$
\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}
$$

推导过程：

$$
\frac{\partial C}{\partial w_{j k}^{l}}=\frac{\partial C}{\partial z_{j}^{l}} \cdot \frac{\partial z_{j}^{l}}{\partial w_{j k}^{l}}=\delta_{j}^{l} \cdot \frac{\partial\left(w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}
$$

**公式4（计算偏置的梯度）：**

$$
\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}
$$

推导过程：

$$
\frac{\partial C}{\partial b_{j}^{l}}=\frac{\partial C}{\partial z_{j}^{l}} \cdot \frac{\partial z_{j}^{l}}{\partial b_{j}^{l}}=\delta_{j}^{l} \cdot \frac{\partial\left(w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)}{\partial b_{j}^{l}}=\delta_{j}^{l}
$$



**注**：其中向量化表示的第l层的$W^l,b^l$的梯度如下：
$$
\frac{\partial J(W, b, x, y)}{\partial W^{l}}=\delta^{l}\left(a^{l-1}\right)^{T}
$$

$$
\frac{\partial J(W, b, x, y)}{\partial b^{l}}=\delta^{l}
$$



# **4. 反向传播算法伪代码**

- **输入训练集**

- **对于训练集中的每个样本x，设置输入层（Input layer）对应的激活值$a^{1}$：**

- **前向传播：**

$$
z^{l}=w^{l} a^{l-1}+b^{l}, a^{l}=\sigma\left(z^{l}\right)
$$

  + **计算输出层产生的错误：**

$$
\delta^{L}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right)
$$

  + **反向传播错误：**

$$
\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)
$$

 + **使用梯度下降（gradient descent），训练参数：**

$$
w^{l} \rightarrow w^{l}-\frac{\eta}{m} \sum_{x} \delta^{x, l}\left(a^{x, l-1}\right)^{T}
$$

$$
b^{l} \rightarrow b^{l}-\frac{\eta}{m} \sum_{x} \delta^{x, l}
$$



转自：[反向传播算法（过程及公式推导）](https://blog.csdn.net/u014313009/article/details/51039334 )

另可参考：

[5分钟看懂反向传播算法(Backpropogation) |李宏毅机器学习【7】](https://blog.csdn.net/qq_36459893/article/details/82796304)

[刘建平Pinard - 深度神经网络（DNN）反向传播算法(BP)](http://www.cnblogs.com/pinard/p/6422831.html)

[矩阵求导术（上）](https://zhuanlan.zhihu.com/p/24709748)

[刘建平Pinard - 机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则](https://www.cnblogs.com/pinard/p/10825264.html)

视频：

[李宏毅机器学习(2016)](https://www.bilibili.com/video/av9770190/?p=7)

[李宏毅机器学习2019(国语)](https://www.bilibili.com/video/av46561029/)

