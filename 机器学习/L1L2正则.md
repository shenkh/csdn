转自: [深入理解L1、L2正则化](https://zhuanlan.zhihu.com/p/29360425)



正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为：

![\tilde{J}\left( w;X,y \right) =J \left( w;X,y \right)+\alpha\Omega\left( w \right)](https://www.zhihu.com/equation?tex=%5Ctilde%7BJ%7D%5Cleft%28+w%3BX%2Cy+%5Cright%29+%3DJ+%5Cleft%28+w%3BX%2Cy+%5Cright%29%2B%5Calpha%5COmega%5Cleft%28+w+%5Cright%29)

式中 ![X](https://www.zhihu.com/equation?tex=X) 、 ![y](https://www.zhihu.com/equation?tex=y)为训练样本和相应标签， ![w](https://www.zhihu.com/equation?tex=w) 为权重系数向量； ![J\left( \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+%5Cright%29) 为目标函数， ![\Omega\left( w \right)](https://www.zhihu.com/equation?tex=%5COmega%5Cleft%28+w+%5Cright%29) 即为惩罚项，可理解为模型“规模”的某种度量；参数![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 控制控制正则化强弱。不同的 ![\Omega](https://www.zhihu.com/equation?tex=%5COmega) 函数对权重 ![w](https://www.zhihu.com/equation?tex=w) 的最优解有不同的偏好，因而会产生不同的正则化效果。最常用的 ![\Omega](https://www.zhihu.com/equation?tex=%5COmega) 函数有两种，即 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D)范数和 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数，相应称之为 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化和 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化。此时有：

![l_{1}:\Omega\left( w \right)=\left| \left| w \right|\right|_{1}=\sum_{i}{\left| w_{i} \right|}](https://www.zhihu.com/equation?tex=l_%7B1%7D%3A%5COmega%5Cleft%28+w+%5Cright%29%3D%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D%3D%5Csum_%7Bi%7D%7B%5Cleft%7C+w_%7Bi%7D+%5Cright%7C%7D)

![l_{2}:\Omega\left( w \right)=\left| \left| w \right|\right|_{2}^{2}=\sum_{i}{w_{i}^{2}}](https://www.zhihu.com/equation?tex=l_%7B2%7D%3A%5COmega%5Cleft%28+w+%5Cright%29%3D%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5E%7B2%7D%3D%5Csum_%7Bi%7D%7Bw_%7Bi%7D%5E%7B2%7D%7D)

本文将从不同角度详细说明 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的推导、求解过程，并对 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 范数产生稀疏性效果的本质予以解释。

## 一、 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化来源推导

可从带约束条件的优化求解和最大后验概率两种思路来推导 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化，下面将予以详细分析。

## 1、正则化理解之基于约束条件的最优化

对于模型权重系数 ![w](https://www.zhihu.com/equation?tex=w) 求解是通过最小化目标函数实现的，即求解：

![\min_{w }{J\left( w; X, y \right)} ](https://www.zhihu.com/equation?tex=%5Cmin_%7Bw+%7D%7BJ%5Cleft%28+w%3B+X%2C+y+%5Cright%29%7D+)

我们知道，模型的复杂度可用VC维来衡量。通常情况下，模型VC维与系数 ![w](https://www.zhihu.com/equation?tex=w) 的个数成线性关系：即 ![w](https://www.zhihu.com/equation?tex=w) 数量越多，VC维越大，模型越复杂。因此，为了限制模型的复杂度，很自然的思路是减少系数 ![w](https://www.zhihu.com/equation?tex=w) 的个数，即让 ![w](https://www.zhihu.com/equation?tex=w) 向量中一些元素为0或者说限制 ![w](https://www.zhihu.com/equation?tex=w) 中非零元素的个数。为此，我们可在原优化问题中加入一个约束条件：

![\begin{gather} \min_{w }{J\left( w; X, y \right)}\\ s.t. \left| \left| w \right|\right|_{0}\leq C \end{gather}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgather%7D+%5Cmin_%7Bw+%7D%7BJ%5Cleft%28+w%3B+X%2C+y+%5Cright%29%7D%5C%5C+s.t.+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B0%7D%5Cleq+C+%5Cend%7Bgather%7D)

![\left|\left| \cdot \right|\right|_{0}](https://www.zhihu.com/equation?tex=%5Cleft%7C%5Cleft%7C+%5Ccdot+%5Cright%7C%5Cright%7C_%7B0%7D) 范数表示向量中非零元素的个数。但由于该问题是一个NP问题，不易求解，为此我们需要稍微“放松”一下约束条件。为了达到近似效果，我们不严格要求某些权重 ![w](https://www.zhihu.com/equation?tex=w) 为0，而是要求权重 ![w](https://www.zhihu.com/equation?tex=w) 应接近于0，即尽量小。从而可用 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数来近似 ![l_{0}](https://www.zhihu.com/equation?tex=l_%7B0%7D) 范数，即：

![\begin{gather} \min_{w }{J\left( w; X, y \right)}\\ s.t. \left| \left| w \right|\right|_{1}\leq C \end{gather}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgather%7D+%5Cmin_%7Bw+%7D%7BJ%5Cleft%28+w%3B+X%2C+y+%5Cright%29%7D%5C%5C+s.t.+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D%5Cleq+C+%5Cend%7Bgather%7D) 或 ![\begin{gather} \min_{w }{J\left( w; X, y \right)}\\ s.t. \left| \left| w \right|\right|_{2}\leq C \end{gather}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgather%7D+%5Cmin_%7Bw+%7D%7BJ%5Cleft%28+w%3B+X%2C+y+%5Cright%29%7D%5C%5C+s.t.+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5Cleq+C+%5Cend%7Bgather%7D)

使用 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数时，为方便后续处理，可对 ![\left| \left| w \right|\right|_{2}](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D) 进行平方，此时只需调整 ![C](https://www.zhihu.com/equation?tex=C) 的取值即可。利用拉格朗日算子法，我们可将上述带约束条件的最优化问题转换为不带约束项的优化问题，构造拉格朗日函数：

![L\left( w,\alpha \right)=J \left( w;X,y \right)+\alpha\left(\left| \left| w \right|\right|_{1}-C\right)](https://www.zhihu.com/equation?tex=L%5Cleft%28+w%2C%5Calpha+%5Cright%29%3DJ+%5Cleft%28+w%3BX%2Cy+%5Cright%29%2B%5Calpha%5Cleft%28%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D-C%5Cright%29) 或

![L\left( w,\alpha \right)=J \left( w;X,y \right)+\alpha\left( \left| \left| w \right|\right|_{2}^{2} - C \right)](https://www.zhihu.com/equation?tex=L%5Cleft%28+w%2C%5Calpha+%5Cright%29%3DJ+%5Cleft%28+w%3BX%2Cy+%5Cright%29%2B%5Calpha%5Cleft%28+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5E%7B2%7D+-+C+%5Cright%29)

其中 ![\alpha>0](https://www.zhihu.com/equation?tex=%5Calpha%3E0) ，我们假设 ![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 的最优解为 ![\alpha^{\ast}](https://www.zhihu.com/equation?tex=%5Calpha%5E%7B%5Cast%7D) ，则对拉格朗日函数求最小化等价于：

![\min_{w }J \left( w;X,y \right)+\alpha^{\ast} \left| \left| w \right|\right|_{1}](https://www.zhihu.com/equation?tex=%5Cmin_%7Bw+%7DJ+%5Cleft%28+w%3BX%2Cy+%5Cright%29%2B%5Calpha%5E%7B%5Cast%7D+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D) 或

![\min_{w }J \left( w;X,y \right)+\alpha^{\ast} \left| \left| w \right|\right|_{2}^{2}](https://www.zhihu.com/equation?tex=%5Cmin_%7Bw+%7DJ+%5Cleft%28+w%3BX%2Cy+%5Cright%29%2B%5Calpha%5E%7B%5Cast%7D+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5E%7B2%7D)

可以看出，上式与 ![\min_{w }\tilde{J}\left( w;X,y \right) ](https://www.zhihu.com/equation?tex=%5Cmin_%7Bw+%7D%5Ctilde%7BJ%7D%5Cleft%28+w%3BX%2Cy+%5Cright%29+) 等价。

故此，我们得到对 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的第一种理解：

- ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化等价于在原优化目标函数中增加约束条件 ![\left| \left| w \right|\right|_{1}\leq C](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D%5Cleq+C)
- ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化等价于在原优化目标函数中增加约束条件 ![\left| \left| w \right|\right|_{2}^{2}\leq C](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5E%7B2%7D%5Cleq+C)

## 2、正则化理解之最大后验概率估计

在最大似然估计中，是假设权重 ![w](https://www.zhihu.com/equation?tex=w) 是未知的参数，从而求得对数似然函数：

![l\left( w \right) =\text{log}\left[ P\left( y|X;w \right) \right]=\text{log}\left[ \prod_{i}P\left( y^{i}|x^{i};w \right) \right]](https://www.zhihu.com/equation?tex=l%5Cleft%28+w+%5Cright%29+%3D%5Ctext%7Blog%7D%5Cleft%5B+P%5Cleft%28+y%7CX%3Bw+%5Cright%29+%5Cright%5D%3D%5Ctext%7Blog%7D%5Cleft%5B+%5Cprod_%7Bi%7DP%5Cleft%28+y%5E%7Bi%7D%7Cx%5E%7Bi%7D%3Bw+%5Cright%29+%5Cright%5D)

通过假设 ![y^{i}](https://www.zhihu.com/equation?tex=y%5E%7Bi%7D) 的不同概率分布，即可得到不同的模型。例如若假设 ![y^{i}\sim N\left( w^{T}x^{i}， \sigma^{2} \right)](https://www.zhihu.com/equation?tex=y%5E%7Bi%7D%5Csim+N%5Cleft%28+w%5E%7BT%7Dx%5E%7Bi%7D%EF%BC%8C+%5Csigma%5E%7B2%7D+%5Cright%29) 的高斯分布，则有：

![l\left( w \right) =\text{log}\left[ \prod_{i}\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{\left( y^{i}-w^{T}x^{i} \right)^{2}}{2\sigma^{2}}} \right]=-\frac{1}{2\sigma^{2}}\sum_{i}{\left( y^{i}-w^{T}x^{i} \right)^{2}}+C](https://www.zhihu.com/equation?tex=l%5Cleft%28+w+%5Cright%29+%3D%5Ctext%7Blog%7D%5Cleft%5B+%5Cprod_%7Bi%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D+e%5E%7B-%5Cfrac%7B%5Cleft%28+y%5E%7Bi%7D-w%5E%7BT%7Dx%5E%7Bi%7D+%5Cright%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D+%5Cright%5D%3D-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%5Csum_%7Bi%7D%7B%5Cleft%28+y%5E%7Bi%7D-w%5E%7BT%7Dx%5E%7Bi%7D+%5Cright%29%5E%7B2%7D%7D%2BC)

式中 ![C](https://www.zhihu.com/equation?tex=C) 为常数项，由于常数项和系数项不影响 ![\text{max}\space l\left( w \right)](https://www.zhihu.com/equation?tex=%5Ctext%7Bmax%7D%5Cspace+l%5Cleft%28+w+%5Cright%29) 的解，因而可令 ![J \left( w;X,y \right)=- l\left( w \right)](https://www.zhihu.com/equation?tex=J+%5Cleft%28+w%3BX%2Cy+%5Cright%29%3D-+l%5Cleft%28+w+%5Cright%29) 即可得到线性回归的代价函数。

在最大后验概率估计中，则将权重 ![w](https://www.zhihu.com/equation?tex=w) 看作随机变量，也具有某种分布，从而有：

![ P\left( w|X,y \right) =\frac{P\left( w,X,y \right)}{P\left( X,y \right)} =\frac{P\left( X,y|w \right)P\left( w \right)}{P\left( X,y \right)} \propto {P\left( y|X,w \right)P\left( w \right)}](https://www.zhihu.com/equation?tex=+P%5Cleft%28+w%7CX%2Cy+%5Cright%29+%3D%5Cfrac%7BP%5Cleft%28+w%2CX%2Cy+%5Cright%29%7D%7BP%5Cleft%28+X%2Cy+%5Cright%29%7D+%3D%5Cfrac%7BP%5Cleft%28+X%2Cy%7Cw+%5Cright%29P%5Cleft%28+w+%5Cright%29%7D%7BP%5Cleft%28+X%2Cy+%5Cright%29%7D+%5Cpropto+%7BP%5Cleft%28+y%7CX%2Cw+%5Cright%29P%5Cleft%28+w+%5Cright%29%7D)

同样取对数有：

![\text{MAP}=\text{log}{P\left( y|X,w \right)P\left( w \right)}=\text{log}{P\left( y|X,w \right)}+\text{log}P\left( w \right)](https://www.zhihu.com/equation?tex=%5Ctext%7BMAP%7D%3D%5Ctext%7Blog%7D%7BP%5Cleft%28+y%7CX%2Cw+%5Cright%29P%5Cleft%28+w+%5Cright%29%7D%3D%5Ctext%7Blog%7D%7BP%5Cleft%28+y%7CX%2Cw+%5Cright%29%7D%2B%5Ctext%7Blog%7DP%5Cleft%28+w+%5Cright%29)

可以看出后验概率函数为在似然函数的基础上增加了一项 ![\text{log}P\left( w \right)](https://www.zhihu.com/equation?tex=%5Ctext%7Blog%7DP%5Cleft%28+w+%5Cright%29) 。 ![P\left( w \right)](https://www.zhihu.com/equation?tex=P%5Cleft%28+w+%5Cright%29) 的意义是对权重系数 ![w](https://www.zhihu.com/equation?tex=w) 的概率分布的先验假设，在收集到训练样本 ![\left\{ X,y \right\}](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+X%2Cy+%5Cright%5C%7D) 后，则可根据 ![w](https://www.zhihu.com/equation?tex=w) 在 ![\left\{ X,y \right\}](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+X%2Cy+%5Cright%5C%7D) 下的后验概率对 ![w](https://www.zhihu.com/equation?tex=w) 进行修正，从而做出对 ![w](https://www.zhihu.com/equation?tex=w) 更好地估计。

若假设 ![w_{j}](https://www.zhihu.com/equation?tex=w_%7Bj%7D) 的先验分布为0均值的高斯分布，即 ![w_{j}\sim N\left( 0, \sigma^{2} \right)](https://www.zhihu.com/equation?tex=w_%7Bj%7D%5Csim+N%5Cleft%28+0%2C+%5Csigma%5E%7B2%7D+%5Cright%29) ，则有：

![\text{log}P\left( w \right)=\text{log}\prod_{j}P\left( w_{j} \right)=\text{log}\prod_{j}\left[ \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{\left( w_{j} \right)^{2}}{2\sigma^{2}}} \right]=-\frac{1}{2\sigma^{2}}\sum_{j}{w_{j}^{2}}+C'](https://www.zhihu.com/equation?tex=%5Ctext%7Blog%7DP%5Cleft%28+w+%5Cright%29%3D%5Ctext%7Blog%7D%5Cprod_%7Bj%7DP%5Cleft%28+w_%7Bj%7D+%5Cright%29%3D%5Ctext%7Blog%7D%5Cprod_%7Bj%7D%5Cleft%5B+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D+e%5E%7B-%5Cfrac%7B%5Cleft%28+w_%7Bj%7D+%5Cright%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D+%5Cright%5D%3D-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%7D%5Csum_%7Bj%7D%7Bw_%7Bj%7D%5E%7B2%7D%7D%2BC%27)

可以看到，在高斯分布下 ![\text{log}P\left( w \right)](https://www.zhihu.com/equation?tex=%5Ctext%7Blog%7DP%5Cleft%28+w+%5Cright%29) 的效果等价于在代价函数中增加 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则项。

若假设 ![w_{j}](https://www.zhihu.com/equation?tex=w_%7Bj%7D) 服从均值为0、参数为 ![a](https://www.zhihu.com/equation?tex=a) 的拉普拉斯分布，即：

![P\left( w_{j} \right)=\frac{1}{\sqrt{2a}}e^{\frac{-\left| w_{j}\right|}{a}}](https://www.zhihu.com/equation?tex=P%5Cleft%28+w_%7Bj%7D+%5Cright%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2a%7D%7De%5E%7B%5Cfrac%7B-%5Cleft%7C+w_%7Bj%7D%5Cright%7C%7D%7Ba%7D%7D)

则有：

![\text{log}P\left( w \right)=\text{log} \prod_{j}\frac{1}{\sqrt{2a}}e^{\frac{-\left| w_{j}\right|}{a}}=-\frac{1}{a}\sum_{j}{\left| w_{j}\right|}+C'](https://www.zhihu.com/equation?tex=%5Ctext%7Blog%7DP%5Cleft%28+w+%5Cright%29%3D%5Ctext%7Blog%7D+%5Cprod_%7Bj%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2a%7D%7De%5E%7B%5Cfrac%7B-%5Cleft%7C+w_%7Bj%7D%5Cright%7C%7D%7Ba%7D%7D%3D-%5Cfrac%7B1%7D%7Ba%7D%5Csum_%7Bj%7D%7B%5Cleft%7C+w_%7Bj%7D%5Cright%7C%7D%2BC%27)

可以看到，在拉普拉斯分布下 ![\text{log}P\left( w \right)](https://www.zhihu.com/equation?tex=%5Ctext%7Blog%7DP%5Cleft%28+w+%5Cright%29) 的效果等价于在代价函数中增加 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则项。

故此，我们得到对于 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的第二种理解：

- ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化可通过假设权重 ![w](https://www.zhihu.com/equation?tex=w) 的先验分布为拉普拉斯分布，由最大后验概率估计导出；
- ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化可通过假设权重 ![w](https://www.zhihu.com/equation?tex=w) 的先验分布为高斯分布，由最大后验概率估计导出。

## 二、 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化效果分析

本文将从直观分析和理论推导两个角度来说明 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的效果。

## 1、直观理解

考虑带约束条件的优化解释，对 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化为：

![\begin{gather} \min_{w }{J\left( w; X, y \right)}\\ s.t. \left| \left| w \right|\right|_{2}\leq C \end{gather}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgather%7D+%5Cmin_%7Bw+%7D%7BJ%5Cleft%28+w%3B+X%2C+y+%5Cright%29%7D%5C%5C+s.t.+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5Cleq+C+%5Cend%7Bgather%7D)

该问题的求解示意图如下所示：

![img](https://pic2.zhimg.com/80/v2-7431d8a79deec5d0ab3193b6a3611b95_hd.jpg)

图中椭圆为原目标函数 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 的一条等高线，圆为半径 ![\sqrt{C}](https://www.zhihu.com/equation?tex=%5Csqrt%7BC%7D) 的![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数球。由于约束条件的限制， ![w](https://www.zhihu.com/equation?tex=w) 必须位于 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数球内。考虑边界上的一点 ![w](https://www.zhihu.com/equation?tex=w) ，图中蓝色箭头为 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 在该处的梯度方向 ![\nabla J\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+J%5Cleft%28+w+%5Cright%29) ，红色箭头为 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数球在该处的法线方向。由于 ![w](https://www.zhihu.com/equation?tex=w) 不能离开边界（否则违反约束条件），因而在使用梯度下降法更新 ![w](https://www.zhihu.com/equation?tex=w) 时，只能朝 ![\nabla J\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+J%5Cleft%28+w+%5Cright%29) 在 范数球上 ![w](https://www.zhihu.com/equation?tex=w) 处的切线方向更新，即图中绿色箭头的方向。如此 ![w](https://www.zhihu.com/equation?tex=w) 将沿着边界移动，当 ![\nabla J\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+J%5Cleft%28+w+%5Cright%29) 与范数球上 ![w](https://www.zhihu.com/equation?tex=w) 处的法线平行时，此时 ![\nabla J\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+J%5Cleft%28+w+%5Cright%29) 在切线方向的分量为0， ![w](https://www.zhihu.com/equation?tex=w) 将无法继续移动，从而达到最优解 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D)（图中红色点所示）。

对于 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化：

![\begin{gather} \min_{w }{J\left( w; X, y \right)}\\ s.t. \left| \left| w \right|\right|_{1}\leq C \end{gather}](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgather%7D+%5Cmin_%7Bw+%7D%7BJ%5Cleft%28+w%3B+X%2C+y+%5Cright%29%7D%5C%5C+s.t.+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D%5Cleq+C+%5Cend%7Bgather%7D)

同理，其求解示意图如下所示：

![img](https://pic1.zhimg.com/80/v2-592216faffaa338fc792430a538afefc_hd.jpg)

其主要差别在于 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 范数球的形状差异。由于此时每条边界上 ![w](https://www.zhihu.com/equation?tex=w) 的切线和法线方向保持不变，在图中 ![w](https://www.zhihu.com/equation?tex=w) 将一直朝着 ![\nabla J\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+J%5Cleft%28+w+%5Cright%29) 在切线方向的分量沿着边界向左上移动。当 ![w](https://www.zhihu.com/equation?tex=w) 跨过顶点到达 ![w'](https://www.zhihu.com/equation?tex=w%27) 时， ![\nabla J\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+J%5Cleft%28+w+%5Cright%29) 在切线方向的分量变为右上方，因而 ![w](https://www.zhihu.com/equation?tex=w) 将朝右上方移动。最终， ![w](https://www.zhihu.com/equation?tex=w) 将稳定在顶点处，达到最优解 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) 。此时，可以看到 ![w_{1}=0](https://www.zhihu.com/equation?tex=w_%7B1%7D%3D0) ，这也就是采用 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 范数会使![w](https://www.zhihu.com/equation?tex=w)产生稀疏性的原因。

以上分析虽是基于二维的情况，但不难将其推广到多维情况，其主要目的是为了直观地说明 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 、 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化最优解的差异，以及 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 范数为什么为产生稀疏性。

## 2、理论分析

假设原目标函数 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 的最优解为 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) ，并假设其为二阶可导，将 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 在 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) 处进行二阶泰勒展开有：

![\hat{J} \left( w \right)=J\left( w^{*} \right)+\frac{1}{2}\left( w-w^{*} \right)^{T}H\left( w-w^{*} \right)](https://www.zhihu.com/equation?tex=%5Chat%7BJ%7D+%5Cleft%28+w+%5Cright%29%3DJ%5Cleft%28+w%5E%7B%2A%7D+%5Cright%29%2B%5Cfrac%7B1%7D%7B2%7D%5Cleft%28+w-w%5E%7B%2A%7D+%5Cright%29%5E%7BT%7DH%5Cleft%28+w-w%5E%7B%2A%7D+%5Cright%29)

式中 ![H](https://www.zhihu.com/equation?tex=H) 为 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 在 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) 处的Hessian矩阵，注意 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) 为 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 的最优解，其一阶导数为0，因而式中无一阶导数项。 ![\hat{J} \left( w \right)](https://www.zhihu.com/equation?tex=%5Chat%7BJ%7D+%5Cleft%28+w+%5Cright%29) 取得最小值时有：

![\nabla _{w} \hat{J}\left( w \right)=H\left( w-w^{*} \right)=0](https://www.zhihu.com/equation?tex=%5Cnabla+_%7Bw%7D+%5Chat%7BJ%7D%5Cleft%28+w+%5Cright%29%3DH%5Cleft%28+w-w%5E%7B%2A%7D+%5Cright%29%3D0)

由于 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的目标函数为在 ![J\left( w \right)](https://www.zhihu.com/equation?tex=J%5Cleft%28+w+%5Cright%29) 中添加 ![\Omega\left( w \right)=\frac{1}{2}\alpha \left| \left| w \right|\right|_{2}^{2}=\frac{1}{2}\alpha w^{T}w](https://www.zhihu.com/equation?tex=%5COmega%5Cleft%28+w+%5Cright%29%3D%5Cfrac%7B1%7D%7B2%7D%5Calpha+%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5E%7B2%7D%3D%5Cfrac%7B1%7D%7B2%7D%5Calpha+w%5E%7BT%7Dw) ，因而有：

![\nabla _{w} \tilde{J}\left( w \right)=\nabla _{w} \hat{J}\left( w \right)+ \nabla _{w}\Omega\left( w \right)=H\left( w-w^{*} \right)+\alpha w](https://www.zhihu.com/equation?tex=%5Cnabla+_%7Bw%7D+%5Ctilde%7BJ%7D%5Cleft%28+w+%5Cright%29%3D%5Cnabla+_%7Bw%7D+%5Chat%7BJ%7D%5Cleft%28+w+%5Cright%29%2B+%5Cnabla+_%7Bw%7D%5COmega%5Cleft%28+w+%5Cright%29%3DH%5Cleft%28+w-w%5E%7B%2A%7D+%5Cright%29%2B%5Calpha+w)

设其最优解为 ![\tilde{w}](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) ，则有：

![H\left( \tilde w-w^{*} \right)+\alpha \tilde w=0](https://www.zhihu.com/equation?tex=H%5Cleft%28+%5Ctilde+w-w%5E%7B%2A%7D+%5Cright%29%2B%5Calpha+%5Ctilde+w%3D0)

![\tilde w=\left( H+\alpha I \right)^{-1}H w^{*}](https://www.zhihu.com/equation?tex=%5Ctilde+w%3D%5Cleft%28+H%2B%5Calpha+I+%5Cright%29%5E%7B-1%7DH+w%5E%7B%2A%7D)

由于 ![H](https://www.zhihu.com/equation?tex=H) 是对称矩阵，可对其作特征值分解，即 ![H=Q\Lambda Q^{T}](https://www.zhihu.com/equation?tex=H%3DQ%5CLambda+Q%5E%7BT%7D) ，其中 ![Q](https://www.zhihu.com/equation?tex=Q) 为正交矩阵，且每一列为 ![H](https://www.zhihu.com/equation?tex=H) 的特征向量，代入上式有：

![\tilde w=Q\left( \Lambda+\alpha I \right)^{-1} \Lambda Q^{T} w^{*}](https://www.zhihu.com/equation?tex=%5Ctilde+w%3DQ%5Cleft%28+%5CLambda%2B%5Calpha+I+%5Cright%29%5E%7B-1%7D+%5CLambda+Q%5E%7BT%7D+w%5E%7B%2A%7D)

![\Lambda](https://www.zhihu.com/equation?tex=%5CLambda) 为对角矩阵，且对角线元素为 ![H](https://www.zhihu.com/equation?tex=H) 的特征值 ![\lambda_{j}](https://www.zhihu.com/equation?tex=%5Clambda_%7Bj%7D) 。

![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) 可在 ![Q](https://www.zhihu.com/equation?tex=Q) 为正交基上作线性展开，由上式可知 ![\tilde w](https://www.zhihu.com/equation?tex=%5Ctilde+w) 为 ![w^{*}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D) 在 ![H](https://www.zhihu.com/equation?tex=H) 的每个特征向量上的分量以 ![\frac{\lambda_{j}}{\lambda_{j}+\alpha}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda_%7Bj%7D%7D%7B%5Clambda_%7Bj%7D%2B%5Calpha%7D) 比例放缩得到。若 ![\lambda_{j}\gg\alpha](https://www.zhihu.com/equation?tex=%5Clambda_%7Bj%7D%5Cgg%5Calpha) ，则 ![w^{*}_{j}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D_%7Bj%7D) 受正则化的影响较小；若 ![\lambda_{j}\ll\alpha](https://www.zhihu.com/equation?tex=%5Clambda_%7Bj%7D%5Cll%5Calpha) ，则 ![w^{*}_{j}](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D_%7Bj%7D)受正则化的影响较大，将收缩到接近于0的值。同时，若 ![w^{*}_{j}\ne0](https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D_%7Bj%7D%5Cne0) ，则 ![\tilde w_{j}\ne0](https://www.zhihu.com/equation?tex=%5Ctilde+w_%7Bj%7D%5Cne0) ，因而 ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化不会产生稀疏性的效果。

对于 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化，只需将 ![\Omega\left( w \right)](https://www.zhihu.com/equation?tex=%5COmega%5Cleft%28+w+%5Cright%29) 替换为 ![w](https://www.zhihu.com/equation?tex=w) 的 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 范数，同理可以得到：

![\nabla _{w} \tilde{J}\left( w \right)=\nabla _{w} \hat{J}\left( w \right)+\nabla _{w} \Omega\left( w \right)=H\left( w-w^{*} \right)+\alpha \text{sign}\left( w \right)](https://www.zhihu.com/equation?tex=%5Cnabla+_%7Bw%7D+%5Ctilde%7BJ%7D%5Cleft%28+w+%5Cright%29%3D%5Cnabla+_%7Bw%7D+%5Chat%7BJ%7D%5Cleft%28+w+%5Cright%29%2B%5Cnabla+_%7Bw%7D+%5COmega%5Cleft%28+w+%5Cright%29%3DH%5Cleft%28+w-w%5E%7B%2A%7D+%5Cright%29%2B%5Calpha+%5Ctext%7Bsign%7D%5Cleft%28+w+%5Cright%29)

其最优解满足：

![H\left( \tilde w-w^{*} \right)+\alpha \text{sign}\left( \tilde w \right)=0](https://www.zhihu.com/equation?tex=H%5Cleft%28+%5Ctilde+w-w%5E%7B%2A%7D+%5Cright%29%2B%5Calpha+%5Ctext%7Bsign%7D%5Cleft%28+%5Ctilde+w+%5Cright%29%3D0)

为了简化讨论，我们假设 ![H](https://www.zhihu.com/equation?tex=H) 为对角阵，即 ![H=\text{diag} \left[ H_{11}, H_{22},...,H_{nn}\right]](https://www.zhihu.com/equation?tex=H%3D%5Ctext%7Bdiag%7D+%5Cleft%5B+H_%7B11%7D%2C+H_%7B22%7D%2C...%2CH_%7Bnn%7D%5Cright%5D) ， ![H_{jj}>0](https://www.zhihu.com/equation?tex=H_%7Bjj%7D%3E0)。此时 ![w](https://www.zhihu.com/equation?tex=w) 的不同分量之间没有相关性，该假设可通过对输入特征进行预处理（例如使用PCA）得到，此时 ![\tilde w](https://www.zhihu.com/equation?tex=%5Ctilde+w) 的解为：

![\tilde w_{j}=\text{sign} \space \left( w^{*}_{j} \right)\max \left\{ \left| w^{*}_{j} \right|-\frac{\alpha}{H_{jj}} , 0 \right\}](https://www.zhihu.com/equation?tex=%5Ctilde+w_%7Bj%7D%3D%5Ctext%7Bsign%7D+%5Cspace+%5Cleft%28+w%5E%7B%2A%7D_%7Bj%7D+%5Cright%29%5Cmax+%5Cleft%5C%7B+%5Cleft%7C+w%5E%7B%2A%7D_%7Bj%7D+%5Cright%7C-%5Cfrac%7B%5Calpha%7D%7BH_%7Bjj%7D%7D+%2C+0+%5Cright%5C%7D)

当 ![ \left| w^{*}_{j} \right|\leq\frac{\alpha}{H_{jj}}](https://www.zhihu.com/equation?tex=+%5Cleft%7C+w%5E%7B%2A%7D_%7Bj%7D+%5Cright%7C%5Cleq%5Cfrac%7B%5Calpha%7D%7BH_%7Bjj%7D%7D) 时，可知 ![\tilde w_{j}=0](https://www.zhihu.com/equation?tex=%5Ctilde+w_%7Bj%7D%3D0) ，因而 ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化会使最优解的某些元素为0，从而产生稀疏性； ![ \left| w^{*}_{j} \right| > \frac{\alpha}{H_{jj}}](https://www.zhihu.com/equation?tex=+%5Cleft%7C+w%5E%7B%2A%7D_%7Bj%7D+%5Cright%7C+%3E+%5Cfrac%7B%5Calpha%7D%7BH_%7Bjj%7D%7D) 时， ![\tilde w_{j}](https://www.zhihu.com/equation?tex=%5Ctilde+w_%7Bj%7D) 会在原有最优解上偏移一个常数值。

综上， ![l_{2}](https://www.zhihu.com/equation?tex=l_%7B2%7D) 正则化的效果是对原最优解的每个元素进行不同比例的放缩； ![l_{1}](https://www.zhihu.com/equation?tex=l_%7B1%7D) 正则化则会使原最优解的元素产生不同量的偏移，并使某些元素为0，从而产生稀疏性。



参考文献：

1. Ian Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning.
2. Hsuan-Tien Lin. Machine Learning Foundations Lecture 14.