利用梯度下降法拟合线性方程

```python
# -*-coding:utf-8 -*-
import numpy as np
import pandas as pd

class LinearEstimator():

    def __init__(self, lr=0.01):
        self._lr = lr
    
    def _activate(self, z):
        return z
        # return 1./(1+np.exp(-z))
    
    def fit(self, X, y):
        m, n = X.shape
        self._weight = np.ones((n, 1))

        for _ in range(20):
            h = self._activate(X @ self._weight)
            delta =  h - y
            loss = 1/m * delta.transpose() @ delta
            print(loss)
            self._weight = self._weight - self._lr * np.transpose(X) @ delta
             
    def predict(self, X):
        return self._activate(X @ self._weight) > 0.5


if __name__ == "__main__":
    # 生成数据
    X = np.random.randn(100, 1)
    w = np.array([3])
    y = (X @ w).reshape(-1, 1) + np.random.rand(100, 1)
    plt.scatter(X, y, s=10, c='g')
    plt.show()

    linear = LinearEstimator()
    linear.fit(X, y)
    print(linear._weight)

```



![在这里插入图片描述](https://img-blog.csdnimg.cn/20190510162855935.png)

# 最小二乘法的矩阵法解法

矩阵法比代数法要简洁，且矩阵运算可以取代循环，所以现在很多书和机器学习库都是用的矩阵法来做最小二乘法。

这里用多元线性回归例子来描述矩阵法解法。

假设函数$h_\theta\left(x_{1}, x_{2}, \dots x_{n}\right)=\theta_{0}+\theta_{1} x_{1}+\ldots+\theta_{n-1} x_{n-1}$的矩阵表达方式为：

$$
h_{\theta}(\mathbf{x})=\mathbf{X} \theta
$$

其中， 假设函数$h \theta(\mathbf{X})$为mx1的向量$\theta$为nx1的向量，里面有n个代数法的模型参数。$\mathbf{X}$为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。

损失函数定义为$J(\theta)=\frac{1}{2}(\mathbf{X} \theta-\mathbf{Y})^{T}(\mathbf{X} \theta-\mathbf{Y})$

其中$\mathbf{Y}$是样本的输出向量，维度为mx1. $frac{1}{2}$在这主要是为了求导后系数为1，方便计算。

根据最小二乘法的原理，我们要对这个损失函数对$\theta$向量求导取0。结果如下式：
$$
\frac{\partial}{\partial \theta} J(\theta)=\mathbf{X}^{T}(\mathbf{X} \theta-\mathbf{Y})=0
$$
这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。

公式1：

$\frac{\partial}{\partial x}\left(\mathbf{x}^{\mathbf{T}} \mathbf{x}\right)=2 \mathbf{x}$，$\mathbf{x}$为向量

公式2：

$\nabla_{X} f(A X+B)=A^{T} \nabla_{Y} f, \quad Y=A X+B$，$f(Y)$为标量

上述求导等式整理后可得：
$$
\mathbf{X}^{\mathbf{T}} \mathbf{X} \boldsymbol{\theta}=\mathbf{X}^{\mathbf{T}} \mathbf{Y}
$$
两边同时左乘$\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-1}$可得：
$$
\theta=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathbf{T}} \mathbf{Y}
$$
这样我们就一下子求出了$\theta$向量表达式的公式，免去了代数法一个个去求导的麻烦。只要给了数据,我们就可以用$\theta=\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathbf{T}} \mathbf{Y}$算出$\theta$。

**转自：[最小二乘法小结](https://www.cnblogs.com/pinard/p/5976811.html)**

